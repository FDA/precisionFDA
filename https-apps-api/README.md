# PFDA HTTPS apps module

## Run from Docker (via docker-compose)

Use this if you want to run nodejs-api and nodejs-worker as services while developing other sections of PFDA (client, Rails app) or for local testing. Docker builds two images for worker and api (with just different CMD). If you want to make any changes to nodejs code or `.env` file or `key.pem/cert.pem` the image has to be rebuilt. We do not use "live-reload" via Docker volume-bind because of variety of issues (the reload is very slow on MacOX, problem with shadowing `node_modules` folder). If you wanna make changes to the nodejs code, follow the setup in "Development" section.

Files `.env` and `key.pem` and `cert.pem` are required to successfully build the images. The `.env` file can be created from `.env.example` or retrieved from a colleague, for the cert files follow instructions below.

Follow these instructions from PFDA root folder:

```bash
$ docker-compose up -d nodejs-api
$ docker-compose up -d nodejs-worker
```

## Development

- make sure you have the components from root `docker-compose` up and running. Specifically, `db` and `redis` service.
- make sure you have the basic `.env` file prepared. The file has to be located in `./https-apps-api` folder. You can copy it from PFDA root folder or ask your team member for one :)
- make sure ENV vars are ready for local development instead of containerized run. Namely make sure `db` and `redis` addresses work (notice the host change):

```bash
# EXAMPLE - WHEN RUNNING VIA DOCKER
NODE_DATABASE_URL="mysql://root:password@db:3306/precision-fda"

# EXAMPLE - WHEN RUNNING OUT OF DOCKER
NODE_DATABASE_URL="mysql://root:password@localhost:3306/precision-fda"
```

- make sure paths to key.pem and cert.pem are correct (you might need to change the route from `./key.pem` to `../key.pem`)
- `https-apps-api` folder contains a Makefile with all the necessary commands.
- run from within the folder:

```bash
$ make install
$ make run-dev # runs the API
$ make run-worker-dev # runs the worker process
```

### Run tests

Tests should use the configuration environment specified in `@shared/config/envs`.

```bash
$ make test-api
```

Make sure test database ENV vars are configured properly:

```bash
# EXAMPLE
DATABASE_TEST_NAME=precisionfda-test
DATABASE_TEST_URL=mysql://root:password@localhost:3306/precisionfda-test
```

### Certificate

- the API uses self-generated certificate.
- The certificates can be either sent to you from your fellow colleague or you can generate them using mkcert (https://github.com/FiloSottile/mkcert).
- Run following steps from the root project folder:

```bash
$ brew install mkcert
$ mkcert -install
$ mkcert localhost
# rename or otherwise make sure naming matches your .env file
$ mv localhost.pem cert.pem
$ mv localhost-key.pem key.pem
```

### Environments

Running nodejs in different environments will result in different default configurations. These are
defined in `packages/src/shared/config/envs`

### Endpoints

- **Context**
  - all endpoints require `user context` provided to them
  - three values in request query:
    1. `id` - user id from the local database
    2. `accessToken` - generated by the DNANexus Platform
    3. `dxuser` - Platform user handle
  - the validation should walk you through the details
- RUN HTTPS app - `POST /apps/:app_dxid/run`
  - input (request body):
    1. `httpsAppType`: one of `jupyter | custom | ttyd | cloud_workstation`. Currently, only Jupyter apps are supported so this option is required but ignored in the api execution (Jupyter is always used)
    2. `duration`: how long the app should be online (in minutes)
    3. `instanceType`: matches values from the platform (again, for jupyter apps). For example `baseline-2`.
  - it actually calls the Platform and creates jupyter lab instance (or it fails if user token is not valid etc)
  - it also creates a worker task that pings the Platform for updates (only status changes at the moment) so this value can change under the hood. Worker runs every 2 minutes until the execution is terminated.
- LIST HTTPS app executions - `GET /jobs` (currently WIP and unused)
  - input (request query):
    1. `limit` - for pagination (default value 10)
    2. `offset` - pagination (default value 1)
    3. `spaceId` - list jobs in a space
    4. `scope` - used for My Home, can be me, featured, . Ignored if spaceId is present


### Useful Commands for Debugging

To test API calls from console, first store the user context required in an environment variable
`export USER_CONTEXT="id=<USER_ID>&dxuser=<DXUSER>&accessToken=<ACCESS_TOKEN>"`

The access token of a particular user is stored in the `sessions` table


Start workstation
`curl -sk "https://localhost:3001/apps/$APP_ID/run?$USER_CONTEXT"`

List all jobs for a user
`curl -sk "https://localhost:3001/jobs?$USER_CONTEXT" | python -m json.tool`

Describe a job
`curl -sk "https://localhost:3001/jobs/$JOB_ID?$USER_CONTEXT" | python -m json.tool`

Terminate job
`curl -sk --request PATCH "https://localhost:3001/jobs/$JOB_ID/terminate?$USER_CONTEXT"`

Sync workstation files
`curl -sk --request PATCH "https://localhost:3001/jobs/$JOB_ID/syncFiles?$USER_CONTEXT"`

Send email
`curl -sk --request POST "https://localhost:3001/emails/$EMAIL_ID/send?$USER_CONTEXT"`

Start user checkup task
`curl -sk "https://localhost:3001/account/checkup?$USER_CONTEXT" | python -m json.tool`

Stale job report task (admin)
`curl -sk "https://localhost:3001/admin/checkStaleJobs?$USER_CONTEXT" | python -m json.tool`

Debug jobs in bull queue
`curl -sk "https://localhost:3001/debug/queue?$USER_CONTEXT" | python -m json.tool`

`curl -sk "https://localhost:3001/debug/queue/job/$BULLJOB_ID?$USER_CONTEXT" | python -m json.tool`


Cleanup bull queue. If redis is filled up due to failed email jobs, try cleaning up the queue to remove these dead entries
This command returns a list of the removed jobs.

`curl -sk "https://localhost:3001/debug/queue/cleanup?$USER_CONTEXT" | python -m json.tool`

